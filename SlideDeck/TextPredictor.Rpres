TextPredictor
========================================================
My online content:
- Source Code: https://github.com/Smeths/CapStoneProject
- Exploratory Analysis: https://rpubs.com/Smeths/238057
- Application: https://jaysmey.shinyapps.io/ShinyApp/

Useful online resources:
- https://www.youtube.com/user/afigfigueira/playlists?shelf_id=5&view=50&sort=dd
- https://www.youtube.com/playlist?list=PL4LJlvG_SDpxQAwZYtwfXcQr7kGnl9W93

What is TextPredictor?
========================================================

TextPredictor is an text prediction application.This presentation describes:

- How TextPredictor works
- How the App works
- Assessing Performance

How TextPredictor Works
========================================================

1. Data frames of trigrams and bigrams with associated conditional probabilities have been formed from large corpi of twitter, blogs and news sources 
2. Given two words the trigram data frame is searched for a match and the word with the largest probability is returned
3. If there is no match in the trigram data frame, the first word is ignored and bigram dataframe is searched

```{r, echo=FALSE}
trigram_df <- read.csv("../ShinyApp/data/trigram.csv")
tail(trigram_df)
```

How the App works
========================================================

The input section of the app is on the left hand side and output section is on the right. When text is entered into the input section following outputs are displayed:

- A prediction of the next world
- The type of model used to make the prediction
- The associated probability
- Any warnings (such as entering a word not in the apps corpus)
- A section of the underlying dataframe showing alternative predictions

Assessing Performance
========================================================

Perplexity has been used to assess the performance of the model

1. Training set and test set have were formed
2. Conditional probability models are calculated using the training set
3. The perplexity is then calculated as below, using using the sentences s1,..,sm from the test data

$Perplexity = 2^{-l}$

$l = \frac{1}{M} \sum\limits_{i=1}^{m}\log(p(s_{i}))$ (M = Number of words in training set)

A corpus of 5863 words gave a bigram perplexity of 130.2



